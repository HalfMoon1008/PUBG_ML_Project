{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LightGBM Modling in ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. based on ensemble, GBM\n",
    "    - ensemble\n",
    "        - 독립적인 개체들간의 다수결 투표를 통해서 대표 모델 선출(bagging)\n",
    "        - 특정 모델의 output을 다른 모델의 input에 사용하는 방법으로 과중치를 부여하는 방법(boosting)\n",
    "    - GBM (Gradient Boosting Machine)\n",
    "        - Boosting Model에서 사용하는 가중치를 Gradient로 사용한 기법\n",
    "        - 데이터 정규화나 아웃라이어의 별도 처리가 없이 범용적으로 사용할 수 있다. 대표적인 모델 : XGBoost\n",
    "2. 단점 극복을 위한 방법론 채택\n",
    "    - GOSS (Gradient-based One-Side Sampling) : low gradient 데이터를 배제함으로써 size를 줄임\n",
    "    - EFB (Exclusive Feature Bundling) : 서로 배타적인 feature들만 골라 사용함으로써, dimension을 줄임\n",
    "3. 다른 Boosting 알고리즘과 차별화되는 특징\n",
    "    - Categorial Feature의 자동 변환 지원\n",
    "    - 메모리 사용량이 적음\n",
    "    - leaf-wise 트리 분할 방식을 사용 _ 대부분의 트리 기반 알고리즘은 level-wise 트리 분할 방식을 사용\n",
    "    - Data Set 크기가 클 때 뛰어난 성능\n",
    "    - 10,000건 이하의 Data set에서는 과적합이 잘 발생\n",
    "4. 기타 특징\n",
    "    - When adding a new tree node, LightGBM chooses the split point that has the largest gain.\n",
    "5. 정리\n",
    "    - 장점\n",
    "        - 학습하는데 시간이 짧다. (통상 XGBoost 학습속도의 1.3배 ~ 1.5배)\n",
    "        - 메모리 사용량이 상대적으로 적다.\n",
    "        - 대용량 데이터 처리 가능\n",
    "    - 적은 데이터셋에서는 오버피팅 발생 가능 (10,000 이하)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about main parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_leaves\n",
    "    - 하나의 트리가 가질 수 있는 최대 리프 개수\n",
    "    - default : 31\n",
    "    - 클수록 정확도는 높아지지만, 오버피팅 발생 가능\n",
    "    - main parameter to control the complexity of the tree model\n",
    "    - max num : 2^(max_depth)\n",
    "        - For example, when the <ins>max_depth=$7$</ins> the depth-wise tree can get good accuracy, but setting num_leaves to <ins>$127$</ins> may cause over-fitting, and setting it to <ins>$70 or 80$</ins> may get better accuracy than depth-wise.\n",
    "    - Decrease num_leaves to reduce training time<br>\n",
    "<br>\n",
    "- min_data_in_leaf\n",
    "    - 한 리프의 최소 데이터 수 (decision tree의 min_sample_leaf와 동일, 오버피팅 제어)\n",
    "    - default : 20\n",
    "    - 클수록 오버피팅 방지\n",
    "    - Minimum number of observations that must fall into a tree node for it to be added.\n",
    "    - This is a very important parameter to prevent over-fitting in a leaf-wise tree\n",
    "    - Its optimal value depends on the number of training samples and num_leaves\n",
    "    - In practice, setting it to hundreds or thousands is enough for a large dataset.<br>\n",
    "<br>\n",
    "- max_depth\n",
    "    - 트리의 깊이\n",
    "    - default : 1\n",
    "    - 위 두개 파라미터와 결합하여 오버피팅 방지\n",
    "    - -1로 설정하면 제한없이 분기한다.\n",
    "    - feature가 많다면 크게 설정\n",
    "    - 파라미터 설정 시 우선적으로 설정\n",
    "    - controls the maximum distance\n",
    "    - You also can use max_depth to limit the tree depth explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about sub parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min_sum_hessian_in_leaf\n",
    "    - Minimum sum of the Hessian (second derivative of the objective function evaluated for each observation) for observations in a leaf.<br><br>\n",
    "- num_iterations\n",
    "    - 반복 수행하려는 트리의 개수 (너무 크면 오버피팅 발생)\n",
    "    - default : 100\n",
    "    - 일반적으로 100 ~ 1000 값이 좋다.\n",
    "    - controls the number of boosting rounds that will be performed\n",
    "    - If you try changing num_iterations, change the <ins>learning_rate</ins> as well\n",
    "    - Decrease num_iterations to reduce training time.<br><br>\n",
    "- early_stopping_round\n",
    "    - If early stopping is enabled, after each boosting round the model’s training accuracy is evaluated against a validation set that contains data not available to the training process\n",
    "    - For example, early_stopping_round=1 says “the first time accuracy on the validation set does not improve, stop training”.<br><br>\n",
    "- learning_rate\n",
    "    - 부스팅 스탭 반복할 때 학습률. 0~1 사이의 값\n",
    "    - 일반적으로 0.01 ~ 0.1 정도로 맞추고 다른 파라미터를 튜닝한다.\n",
    "    - 나중에 성능을 더 높일 때 learning rate를 더 줄인다.\n",
    "    - default : 0.1<br><br>\n",
    "- objective\n",
    "    - 수치예측이면 regression, 이진분류이면 binary\n",
    "    - <ins>사용하는 데이터셋의 타겟팅 값의 형태에 따라 조정 필요</ins>\n",
    "    - default : regression<br><br>\n",
    "- boosting\n",
    "    - 부스팅 방법 (gbdt: Gradient Boosting DecisionTree / rf: RandomForest)\n",
    "    - default : gbdt\n",
    "        - gbdt : traditional Gradient Boosting Cecision Tree, aliases : gbrt\n",
    "        - rf : Random Forest, aliases : random_forest\n",
    "        - dart : Dropouts meet Multiple Additive Regression Trees\n",
    "        - goss : Gradient - bsed One-side Sampling<br><br>\n",
    "- bagging_fraction\n",
    "    - 데이터 샘플링 비율, 오버피팅 제어\n",
    "    - 범위 : 0 < fraction <= 1, 0이 되지 않도록 해야함\n",
    "    - default : 1.0\n",
    "    - 배깅을 하기 위해서 데이터를 랜덤 샘플링하여 학습에 사용한다.<br><br>\n",
    "- feature_fraction\n",
    "    - 개별 트리 학습 시 무작위로 선택하는 feature의 비율\n",
    "    - 1보다 작다면 LGBM은 매 iteration(tree)마다 다른 featrue를 랜덤하게 추출하여 학습하게 된다.\n",
    "    - 만약 0.8로 값을 설정하면 매 tree를 구성할 때, feature의 80%만 랜덤하게 선택한다.\n",
    "    - 과적합을 방지하기 위해 사용할 수 있으며, 학습 속도가 향상한다.\n",
    "    - default : 1.0 <br><br>\n",
    "- lambda_l1, lambda_l2\n",
    "    - L1 regulation 제어, L2 regulation 제어\n",
    "    - 0.0<br><br>\n",
    "- metric\n",
    "    - 성능평가를 어떤 것으로 할 것인지(auc, l1, l2 등)\n",
    "    - <ins>성능평가를 어떤 것으로 할 것인지 조정 필요</ins>\n",
    "    - \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고링크\n",
    "- <href>https://blog.naver.com/ilovelatale/222298514382</href> : Kaggle case study - LightGBM\n",
    "- <href>https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html</href> : LightGBM - Parameters Tuning\n",
    "- <href>https://for-my-wealthy-life.tistory.com/24</href> : lightGBM이란? 파라미터 설명과 코드 실습\n",
    "- <href>https://greatjoy.tistory.com/72</href> : [LightGBM] LGBM는 어떻게 사용할까? (설치,파라미터튜닝)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defalut setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    #start_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    #end_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/sanghyun/Desktop/Coding/PUBG_ML_Project/data/featured_train_4.csv')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원본 데이터 보존을 위해 train data copy\n",
    "\n",
    "data = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1621395, 46), (405349, 46), (1621395,), (405349,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns='winPlacePerc')\n",
    "y = data['winPlacePerc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LigthGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor MAE : 0.046707\n"
     ]
    }
   ],
   "source": [
    "# LGBMRegressor\n",
    "\n",
    "model_lgb = LGBMRegressor().fit(X_train, y_train)\n",
    "pred4 = model_lgb.predict(X_test)\n",
    "mae = mean_absolute_error(pred4, y_test)\n",
    "print('LGBMRegressor MAE : %f' %mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyperparameterTuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1621395, 46), (405349, 46), (1621395,), (405349,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 학습 데이터 split\n",
    "\n",
    "X = data.drop(columns='winPlacePerc')\n",
    "y = data['winPlacePerc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "\n",
    "## Validation data split\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyun/miniforge3/envs/yeardream/lib/python3.8/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## data scalering\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lgbm = LGBMRegressor()\n",
    "# 학습\n",
    "reg_lgbm.fit(X_train, y_train) # eval_set=[(X_train, y_train),(X_val, y_val)] test.csv 데이터를 사용 할 때는 이  파미를 추가한다. , eval_metric도 추가\n",
    "\n",
    "# 예측\n",
    "pred_train_lgbm = reg_lgbm.predict(X_train) # 학습\n",
    "pred_val_lgbm = reg_lgbm.predict(X_val) # 확인\n",
    "\n",
    "# 오차값\n",
    "mse_train_lgbm = mean_absolute_error(y_train, pred_train_lgbm)\n",
    "mse_val_lgbm = mean_absolute_error(y_val, pred_val_lgbm)\n",
    "\n",
    "# print(\"LGBMRegressor Regression\\t train = %.4f\" % (mse_train_lgbm))\n",
    "print(\"LGBMRegressor Regression\\t train/val = %.4f, %.4f\" % (mse_train_lgbm, mse_val_lgbm))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf3ad073ee54fe618f028f20979c6d93bf2de419467b5c87f8cf480ac49fd2db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yeardream')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
